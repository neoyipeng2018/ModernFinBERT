{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: DataBoost — Targeted Augmentation on Misclassified Samples\n",
    "\n",
    "**Aim:** Train a baseline ModernBERT model, identify what it gets wrong on validation, paraphrase those errors with an LLM (preserving correct labels), add to training set, retrain, and measure improvement.\n",
    "\n",
    "**Steps:**\n",
    "1. Baseline: fine-tune ModernBERT-base on train split\n",
    "2. Error mining: run inference on validation, collect misclassified samples\n",
    "3. LLM paraphrasing: generate N paraphrases per error with correct ground-truth labels\n",
    "4. Augmented training: retrain on original + paraphrased data\n",
    "5. Compare baseline vs DataBoosted accuracy on validation and FPB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%capture\n!pip install -q \"datasets>=3.4.1,<4.0.0\" scikit-learn peft accelerate transformers anthropic"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\nimport torch\nimport torch.nn.functional as F\nfrom transformers import (\n    TrainingArguments, Trainer, AutoModelForSequenceClassification,\n    AutoTokenizer, training_args,\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset, Dataset, concatenate_datasets\nfrom tqdm import tqdm\nimport json\nimport time\nimport os\n\nNUM_CLASSES = 3\nLABEL_NAMES = [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"]\nFPB_SOURCE = 6\nPARAPHRASES_PER_SAMPLE = 3  # number of paraphrases to generate per misclassified sample"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"neoyipeng/financial_reasoning_aggregated\")\n",
    "\n",
    "label_dict = {\"NEUTRAL/MIXED\": 1, \"NEGATIVE\": 0, \"POSITIVE\": 2}\n",
    "\n",
    "ds = ds.filter(lambda x: x[\"task\"] == \"sentiment\")\n",
    "ds = ds.filter(lambda x: x[\"source\"] != FPB_SOURCE)\n",
    "\n",
    "# Keep text and string label for error mining, then convert\n",
    "remove_cols = [c for c in ds[\"train\"].column_names if c not in (\"text\", \"labels\")]\n",
    "ds = ds.map(\n",
    "    lambda ex: {\n",
    "        \"text\": ex[\"text\"],\n",
    "        \"labels\": np.eye(NUM_CLASSES)[label_dict[ex[\"label\"]]],\n",
    "    },\n",
    "    remove_columns=remove_cols,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(ds['train']):,}  |  Val: {len(ds['validation']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_model(train_dataset, val_dataset, output_dir=\"trainer_output\", epochs=10):\n    \"\"\"Train a fresh ModernBERT-base model with LoRA and return model + tokenizer.\"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"answerdotai/ModernBERT-base\",\n        num_labels=NUM_CLASSES,\n        torch_dtype=torch.float32,\n        attn_implementation=\"sdpa\",\n    )\n    model.gradient_checkpointing_enable()\n    tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"Wqkv\", \"out_proj\", \"Wi\", \"Wo\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=TaskType.SEQ_CLS,\n    )\n    model = get_peft_model(model, lora_config)\n    model = model.cuda()\n    model.print_trainable_parameters()\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"])\n\n    train_tok = train_dataset.map(tokenize_function, batched=True)\n    val_tok = val_dataset.map(tokenize_function, batched=True)\n\n    trainer = Trainer(\n        model=model,\n        processing_class=tokenizer,\n        train_dataset=train_tok,\n        eval_dataset=val_tok,\n        args=TrainingArguments(\n            output_dir=output_dir,\n            per_device_train_batch_size=8,\n            gradient_accumulation_steps=4,\n            warmup_steps=10,\n            fp16=True,\n            bf16=False,\n            optim=training_args.OptimizerNames.ADAMW_TORCH,\n            learning_rate=2e-4,\n            weight_decay=0.001,\n            lr_scheduler_type=\"cosine\",\n            seed=3407,\n            num_train_epochs=epochs,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_loss\",\n            greater_is_better=False,\n            save_strategy=\"epoch\",\n            eval_strategy=\"epoch\",\n            logging_strategy=\"epoch\",\n            gradient_checkpointing=True,\n            report_to=\"none\",\n        ),\n        compute_metrics=lambda eval_pred: {\n            \"accuracy\": accuracy_score(\n                eval_pred[1].argmax(axis=-1), eval_pred[0].argmax(axis=-1)\n            )\n        },\n    )\n\n    trainer.train()\n    model = model.cuda().eval()\n    return model, tokenizer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training BASELINE model...\")\n",
    "baseline_model, tokenizer = train_model(\n",
    "    ds[\"train\"], ds[\"validation\"], output_dir=\"trainer_output_baseline\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Mining on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, tokenizer, texts, batch_size=32):\n",
    "    \"\"\"Run inference and return predicted class indices.\"\"\"\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Inference\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch, return_tensors=\"pt\", padding=True,\n",
    "                truncation=True, max_length=512,\n",
    "            )\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            logits = model(**inputs).logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "    return np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts = ds[\"validation\"][\"text\"]\n",
    "val_labels = np.argmax(ds[\"validation\"][\"labels\"], axis=1)\n",
    "\n",
    "val_preds = run_inference(baseline_model, tokenizer, val_texts)\n",
    "\n",
    "val_acc = accuracy_score(val_labels, val_preds)\n",
    "val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "print(f\"\\nBaseline validation accuracy: {val_acc:.4f}\")\n",
    "print(f\"Baseline validation macro F1: {val_f1:.4f}\")\n",
    "print(classification_report(val_labels, val_preds, target_names=LABEL_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect misclassified samples\n",
    "errors = []\n",
    "for i in range(len(val_texts)):\n",
    "    if val_preds[i] != val_labels[i]:\n",
    "        errors.append({\n",
    "            \"text\": val_texts[i],\n",
    "            \"true_label\": int(val_labels[i]),\n",
    "            \"true_label_name\": LABEL_NAMES[val_labels[i]],\n",
    "            \"pred_label\": int(val_preds[i]),\n",
    "            \"pred_label_name\": LABEL_NAMES[val_preds[i]],\n",
    "        })\n",
    "\n",
    "print(f\"\\nMisclassified samples: {len(errors)} / {len(val_texts)} ({len(errors)/len(val_texts):.1%})\")\n",
    "print(f\"\\nError breakdown by true label:\")\n",
    "error_df = pd.DataFrame(errors)\n",
    "print(error_df[\"true_label_name\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 4.5 Baseline Evaluation on FPB (Held-Out)\n\nEvaluate the baseline model on FPB before deletion, so we can measure the DataBoost delta on the same test set.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load FPB test sets\nfpb_50 = load_dataset(\"financial_phrasebank\", \"sentences_50agree\", trust_remote_code=True)[\"train\"]\nfpb_all = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)[\"train\"]\nprint(f\"FPB 50agree: {len(fpb_50):,} samples\")\nprint(f\"FPB allAgree: {len(fpb_all):,} samples\")\n\n# Baseline on FPB 50agree\nbaseline_fpb50_preds = run_inference(baseline_model, tokenizer, fpb_50[\"sentence\"])\nbaseline_fpb50_acc = accuracy_score(fpb_50[\"label\"], baseline_fpb50_preds)\nbaseline_fpb50_f1 = f1_score(fpb_50[\"label\"], baseline_fpb50_preds, average=\"macro\")\nprint(f\"\\nBaseline FPB 50agree — Accuracy: {baseline_fpb50_acc:.4f}  Macro F1: {baseline_fpb50_f1:.4f}\")\nprint(classification_report(fpb_50[\"label\"], baseline_fpb50_preds, target_names=LABEL_NAMES))\n\n# Baseline on FPB allAgree\nbaseline_fpball_preds = run_inference(baseline_model, tokenizer, fpb_all[\"sentence\"])\nbaseline_fpball_acc = accuracy_score(fpb_all[\"label\"], baseline_fpball_preds)\nbaseline_fpball_f1 = f1_score(fpb_all[\"label\"], baseline_fpball_preds, average=\"macro\")\nprint(f\"Baseline FPB allAgree — Accuracy: {baseline_fpball_acc:.4f}  Macro F1: {baseline_fpball_f1:.4f}\")\nprint(classification_report(fpb_all[\"label\"], baseline_fpball_preds, target_names=LABEL_NAMES))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LLM Paraphrasing of Misclassified Samples\n",
    "\n",
    "For each misclassified sample, generate paraphrases using an LLM. The paraphrases keep the **correct ground-truth label**, providing the model with more examples of the patterns it struggles with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set API key — works on Kaggle, Colab, or local\nif \"ANTHROPIC_API_KEY\" not in os.environ:\n    try:\n        from kaggle_secrets import UserSecretsClient\n        os.environ[\"ANTHROPIC_API_KEY\"] = UserSecretsClient().get_secret(\"ANTHROPIC_API_KEY\")\n    except ImportError:\n        import getpass\n        os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter Anthropic API key: \")\n\nfrom anthropic import Anthropic\nclient = Anthropic()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_batch(texts, labels, n_paraphrases=3, batch_size=10):\n",
    "    \"\"\"Generate paraphrases for a batch of texts using Claude.\n",
    "\n",
    "    Args:\n",
    "        texts: List of financial texts to paraphrase.\n",
    "        labels: List of integer labels (ground truth).\n",
    "        n_paraphrases: Number of paraphrases per text.\n",
    "        batch_size: How many texts to send per API call.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'text' and 'label' keys.\n",
    "    \"\"\"\n",
    "    all_paraphrases = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Paraphrasing\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_labels = labels[i : i + batch_size]\n",
    "\n",
    "        # Build prompt with numbered texts\n",
    "        numbered = \"\\n\".join(\n",
    "            f\"{j+1}. [{LABEL_NAMES[lbl]}] {txt}\"\n",
    "            for j, (txt, lbl) in enumerate(zip(batch_texts, batch_labels))\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"You are a financial text paraphrasing assistant. For each numbered financial text below, generate exactly {n_paraphrases} paraphrases that:\n",
    "- Preserve the original meaning and financial sentiment\n",
    "- Use different wording, sentence structure, or phrasing\n",
    "- Stay realistic as financial text (news headlines, earnings reports, analyst commentary)\n",
    "- Keep approximately the same length\n",
    "\n",
    "Return your response as a JSON array of objects, each with \"original_index\" (1-based), \"paraphrase\" (the text), and \"label\" (the sentiment label shown in brackets).\n",
    "\n",
    "Texts to paraphrase:\n",
    "{numbered}\n",
    "\n",
    "Return ONLY valid JSON, no other text.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=4096,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            content = response.content[0].text.strip()\n",
    "            # Strip markdown code fences if present\n",
    "            if content.startswith(\"```\"):\n",
    "                content = content.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0].strip()\n",
    "\n",
    "            paraphrases = json.loads(content)\n",
    "\n",
    "            label_name_to_idx = {name: idx for idx, name in enumerate(LABEL_NAMES)}\n",
    "            for p in paraphrases:\n",
    "                idx = p[\"original_index\"] - 1  # convert to 0-based\n",
    "                lbl = label_name_to_idx.get(p[\"label\"], batch_labels[idx])\n",
    "                all_paraphrases.append({\"text\": p[\"paraphrase\"], \"label\": lbl})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at batch {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        time.sleep(0.5)  # rate limiting\n",
    "\n",
    "    return all_paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_texts = [e[\"text\"] for e in errors]\n",
    "error_labels = [e[\"true_label\"] for e in errors]\n",
    "\n",
    "print(f\"Generating {PARAPHRASES_PER_SAMPLE} paraphrases for {len(errors)} misclassified samples...\")\n",
    "paraphrased = paraphrase_batch(error_texts, error_labels, n_paraphrases=PARAPHRASES_PER_SAMPLE)\n",
    "print(f\"\\nGenerated {len(paraphrased)} paraphrases\")\n",
    "\n",
    "# Show a few examples\n",
    "for p in paraphrased[:6]:\n",
    "    print(f\"  [{LABEL_NAMES[p['label']]}] {p['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Augmented Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert paraphrased data to HF dataset with one-hot labels\n",
    "aug_texts = [p[\"text\"] for p in paraphrased]\n",
    "aug_labels = [np.eye(NUM_CLASSES)[p[\"label\"]].tolist() for p in paraphrased]\n",
    "\n",
    "aug_ds = Dataset.from_dict({\"text\": aug_texts, \"labels\": aug_labels})\n",
    "\n",
    "# Combine original training data + paraphrased augmentations\n",
    "augmented_train = concatenate_datasets([ds[\"train\"], aug_ds]).shuffle(seed=42)\n",
    "\n",
    "print(f\"Original train size:  {len(ds['train']):,}\")\n",
    "print(f\"Augmentation size:    {len(aug_ds):,}\")\n",
    "print(f\"Augmented train size: {len(augmented_train):,}\")\n",
    "print(f\"Augmentation ratio:   {len(aug_ds)/len(ds['train']):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Retrain on Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free baseline model memory\n",
    "del baseline_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Training DATABOOSTED model...\")\n",
    "boosted_model, tokenizer = train_model(\n",
    "    augmented_train, ds[\"validation\"], output_dir=\"trainer_output_boosted\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Baseline vs DataBoosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate DataBoosted model on validation\n",
    "boosted_val_preds = run_inference(boosted_model, tokenizer, val_texts)\n",
    "\n",
    "boosted_val_acc = accuracy_score(val_labels, boosted_val_preds)\n",
    "boosted_val_f1 = f1_score(val_labels, boosted_val_preds, average=\"macro\")\n",
    "\n",
    "print(f\"\\nDataBoosted validation accuracy: {boosted_val_acc:.4f}\")\n",
    "print(f\"DataBoosted validation macro F1:  {boosted_val_f1:.4f}\")\n",
    "print(classification_report(val_labels, boosted_val_preds, target_names=LABEL_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate DataBoosted on FPB 50agree\nboosted_fpb50_preds = run_inference(boosted_model, tokenizer, fpb_50[\"sentence\"])\nboosted_fpb50_acc = accuracy_score(fpb_50[\"label\"], boosted_fpb50_preds)\nboosted_fpb50_f1 = f1_score(fpb_50[\"label\"], boosted_fpb50_preds, average=\"macro\")\nprint(f\"DataBoosted FPB 50agree — Accuracy: {boosted_fpb50_acc:.4f}  Macro F1: {boosted_fpb50_f1:.4f}\")\nprint(classification_report(fpb_50[\"label\"], boosted_fpb50_preds, target_names=LABEL_NAMES))\n\n# Evaluate DataBoosted on FPB allAgree\nboosted_fpball_preds = run_inference(boosted_model, tokenizer, fpb_all[\"sentence\"])\nboosted_fpball_acc = accuracy_score(fpb_all[\"label\"], boosted_fpball_preds)\nboosted_fpball_f1 = f1_score(fpb_all[\"label\"], boosted_fpball_preds, average=\"macro\")\nprint(f\"\\nDataBoosted FPB allAgree — Accuracy: {boosted_fpball_acc:.4f}  Macro F1: {boosted_fpball_f1:.4f}\")\nprint(classification_report(fpb_all[\"label\"], boosted_fpball_preds, target_names=LABEL_NAMES))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "comparison = pd.DataFrame([\n    # Published baselines (in-domain FPB train/test splits)\n    {\"Model\": \"LSTM+ELMo *\",        \"Split\": \"FPB 50agree\",  \"Accuracy\": 0.7500, \"Macro F1\": 0.7000},\n    {\"Model\": \"ULMFit *\",           \"Split\": \"FPB 50agree\",  \"Accuracy\": 0.8300, \"Macro F1\": 0.7900},\n    {\"Model\": \"ProsusAI/finbert *\", \"Split\": \"FPB 50agree\",  \"Accuracy\": 0.8600, \"Macro F1\": 0.8400},\n    {\"Model\": \"FinBERT-FinVocab *\", \"Split\": \"FPB 50agree\",  \"Accuracy\": 0.8720, \"Macro F1\": None},\n    {\"Model\": \"ProsusAI/finbert *\", \"Split\": \"FPB allAgree\", \"Accuracy\": 0.9700, \"Macro F1\": 0.9500},\n    # Our models (FPB held out from training)\n    {\"Model\": \"Baseline (ours)\",    \"Split\": \"Validation\",    \"Accuracy\": val_acc,             \"Macro F1\": val_f1},\n    {\"Model\": \"DataBoosted (ours)\", \"Split\": \"Validation\",    \"Accuracy\": boosted_val_acc,     \"Macro F1\": boosted_val_f1},\n    {\"Model\": \"Baseline (ours)\",    \"Split\": \"FPB 50agree\",   \"Accuracy\": baseline_fpb50_acc,  \"Macro F1\": baseline_fpb50_f1},\n    {\"Model\": \"DataBoosted (ours)\", \"Split\": \"FPB 50agree\",   \"Accuracy\": boosted_fpb50_acc,   \"Macro F1\": boosted_fpb50_f1},\n    {\"Model\": \"Baseline (ours)\",    \"Split\": \"FPB allAgree\",  \"Accuracy\": baseline_fpball_acc, \"Macro F1\": baseline_fpball_f1},\n    {\"Model\": \"DataBoosted (ours)\", \"Split\": \"FPB allAgree\",  \"Accuracy\": boosted_fpball_acc,  \"Macro F1\": boosted_fpball_f1},\n])\n\nprint(\"=\" * 80)\nprint(\"DATABOOST RESULTS\")\nprint(\"=\" * 80)\n\nfor split in [\"Validation\", \"FPB 50agree\", \"FPB allAgree\"]:\n    subset = comparison[comparison[\"Split\"] == split].copy()\n    subset = subset.sort_values(\"Accuracy\", ascending=False)\n    print(f\"\\n--- {split} ---\")\n    print(subset[[\"Model\", \"Accuracy\", \"Macro F1\"]].to_string(index=False, float_format=\"%.4f\"))\n\nprint(f\"\\n{'=' * 80}\")\nprint(\"DATABOOST IMPROVEMENT (Baseline -> DataBoosted)\")\nprint(f\"{'=' * 80}\")\nprint(f\"  Validation:   {boosted_val_acc - val_acc:+.4f} accuracy  |  {boosted_val_f1 - val_f1:+.4f} macro F1\")\nprint(f\"  FPB 50agree:  {boosted_fpb50_acc - baseline_fpb50_acc:+.4f} accuracy  |  {boosted_fpb50_f1 - baseline_fpb50_f1:+.4f} macro F1\")\nprint(f\"  FPB allAgree: {boosted_fpball_acc - baseline_fpball_acc:+.4f} accuracy  |  {boosted_fpball_f1 - baseline_fpball_f1:+.4f} macro F1\")\nprint(f\"\\nAugmentation added {len(aug_ds)} samples ({len(aug_ds)/len(ds['train']):.1%} of original train)\")\nprint(\"\\n* Published baselines trained/tested on in-domain FPB splits (Araci 2019, Yang et al. 2020)\")\nprint(\"  Our models never saw FPB during training — stricter held-out evaluation.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. (Optional) Iterate — Round 2\n",
    "\n",
    "Re-mine errors from the DataBoosted model and augment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run a second round of DataBoost\n",
    "\n",
    "# # Mine errors from boosted model\n",
    "# errors_r2 = []\n",
    "# for i in range(len(val_texts)):\n",
    "#     if boosted_val_preds[i] != val_labels[i]:\n",
    "#         errors_r2.append({\"text\": val_texts[i], \"true_label\": int(val_labels[i])})\n",
    "#\n",
    "# print(f\"Round 2 errors: {len(errors_r2)} (down from {len(errors)})\")\n",
    "#\n",
    "# if len(errors_r2) > 0:\n",
    "#     paraphrased_r2 = paraphrase_batch(\n",
    "#         [e[\"text\"] for e in errors_r2],\n",
    "#         [e[\"true_label\"] for e in errors_r2],\n",
    "#         n_paraphrases=PARAPHRASES_PER_SAMPLE,\n",
    "#     )\n",
    "#     aug_r2 = Dataset.from_dict({\n",
    "#         \"text\": [p[\"text\"] for p in paraphrased_r2],\n",
    "#         \"labels\": [np.eye(NUM_CLASSES)[p[\"label\"]].tolist() for p in paraphrased_r2],\n",
    "#     })\n",
    "#     augmented_train_r2 = concatenate_datasets([augmented_train, aug_r2]).shuffle(seed=42)\n",
    "#\n",
    "#     del boosted_model\n",
    "#     torch.cuda.empty_cache()\n",
    "#\n",
    "#     boosted_model_r2, tokenizer = train_model(\n",
    "#         augmented_train_r2, ds[\"validation\"], output_dir=\"trainer_output_boosted_r2\"\n",
    "#     )\n",
    "#     r2_preds = run_inference(boosted_model_r2, tokenizer, val_texts)\n",
    "#     r2_acc = accuracy_score(val_labels, r2_preds)\n",
    "#     print(f\"Round 2 validation accuracy: {r2_acc:.4f} (delta: {r2_acc - val_acc:+.4f})\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}