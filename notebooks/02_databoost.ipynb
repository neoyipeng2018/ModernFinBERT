{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: DataBoost — Targeted Augmentation on Misclassified Samples\n",
    "\n",
    "**Aim:** Train a baseline ModernBERT model, identify what it gets wrong on validation, paraphrase those errors with an LLM (preserving correct labels), add to training set, retrain, and measure improvement.\n",
    "\n",
    "**Steps:**\n",
    "1. Baseline: fine-tune ModernBERT-base on train split\n",
    "2. Error mining: run inference on validation, collect misclassified samples\n",
    "3. LLM paraphrasing: generate N paraphrases per error with correct ground-truth labels\n",
    "4. Augmented training: retrain on original + paraphrased data\n",
    "5. Compare baseline vs DataBoosted accuracy on validation and FPB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env UNSLOTH_DISABLE_FAST_GENERATION=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install -q datasets scikit-learn anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, FastModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    TrainingArguments, Trainer, AutoModelForSequenceClassification, training_args,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "LABEL_NAMES = [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"]\n",
    "FPB_SOURCE = 6\n",
    "PARAPHRASES_PER_SAMPLE = 3  # number of paraphrases to generate per misclassified sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"neoyipeng/financial_reasoning_aggregated\")\n",
    "\n",
    "label_dict = {\"NEUTRAL/MIXED\": 1, \"NEGATIVE\": 0, \"POSITIVE\": 2}\n",
    "\n",
    "ds = ds.filter(lambda x: x[\"task\"] == \"sentiment\")\n",
    "ds = ds.filter(lambda x: x[\"source\"] != FPB_SOURCE)\n",
    "\n",
    "# Keep text and string label for error mining, then convert\n",
    "remove_cols = [c for c in ds[\"train\"].column_names if c not in (\"text\", \"labels\")]\n",
    "ds = ds.map(\n",
    "    lambda ex: {\n",
    "        \"text\": ex[\"text\"],\n",
    "        \"labels\": np.eye(NUM_CLASSES)[label_dict[ex[\"label\"]]],\n",
    "    },\n",
    "    remove_columns=remove_cols,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(ds['train']):,}  |  Val: {len(ds['validation']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, val_dataset, output_dir=\"trainer_output\", epochs=10):\n",
    "    \"\"\"Train a fresh ModernBERT-base model and return model + tokenizer.\"\"\"\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name=\"answerdotai/ModernBERT-base\",\n",
    "        load_in_4bit=False,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        auto_model=AutoModelForSequenceClassification,\n",
    "        num_labels=NUM_CLASSES,\n",
    "        full_finetuning=True,\n",
    "    )\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"])\n",
    "\n",
    "    train_tok = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_tok = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=32,\n",
    "            gradient_accumulation_steps=1,\n",
    "            warmup_steps=10,\n",
    "            fp16=not torch.cuda.is_bf16_supported(),\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            optim=training_args.OptimizerNames.ADAMW_TORCH,\n",
    "            learning_rate=5e-5,\n",
    "            weight_decay=0.001,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            seed=3407,\n",
    "            num_train_epochs=epochs,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            save_strategy=\"epoch\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "        compute_metrics=lambda eval_pred: {\n",
    "            \"accuracy\": accuracy_score(\n",
    "                eval_pred[1].argmax(axis=-1), eval_pred[0].argmax(axis=-1)\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model = model.cuda().eval()\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training BASELINE model...\")\n",
    "baseline_model, tokenizer = train_model(\n",
    "    ds[\"train\"], ds[\"validation\"], output_dir=\"trainer_output_baseline\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Mining on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, tokenizer, texts, batch_size=32):\n",
    "    \"\"\"Run inference and return predicted class indices.\"\"\"\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Inference\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch, return_tensors=\"pt\", padding=True,\n",
    "                truncation=True, max_length=512,\n",
    "            )\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            logits = model(**inputs).logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "    return np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts = ds[\"validation\"][\"text\"]\n",
    "val_labels = np.argmax(ds[\"validation\"][\"labels\"], axis=1)\n",
    "\n",
    "val_preds = run_inference(baseline_model, tokenizer, val_texts)\n",
    "\n",
    "val_acc = accuracy_score(val_labels, val_preds)\n",
    "val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "print(f\"\\nBaseline validation accuracy: {val_acc:.4f}\")\n",
    "print(f\"Baseline validation macro F1: {val_f1:.4f}\")\n",
    "print(classification_report(val_labels, val_preds, target_names=LABEL_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect misclassified samples\n",
    "errors = []\n",
    "for i in range(len(val_texts)):\n",
    "    if val_preds[i] != val_labels[i]:\n",
    "        errors.append({\n",
    "            \"text\": val_texts[i],\n",
    "            \"true_label\": int(val_labels[i]),\n",
    "            \"true_label_name\": LABEL_NAMES[val_labels[i]],\n",
    "            \"pred_label\": int(val_preds[i]),\n",
    "            \"pred_label_name\": LABEL_NAMES[val_preds[i]],\n",
    "        })\n",
    "\n",
    "print(f\"\\nMisclassified samples: {len(errors)} / {len(val_texts)} ({len(errors)/len(val_texts):.1%})\")\n",
    "print(f\"\\nError breakdown by true label:\")\n",
    "error_df = pd.DataFrame(errors)\n",
    "print(error_df[\"true_label_name\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LLM Paraphrasing of Misclassified Samples\n",
    "\n",
    "For each misclassified sample, generate paraphrases using an LLM. The paraphrases keep the **correct ground-truth label**, providing the model with more examples of the patterns it struggles with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# Set API key — works in Colab or local\n",
    "if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter Anthropic API key: \")\n",
    "\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_batch(texts, labels, n_paraphrases=3, batch_size=10):\n",
    "    \"\"\"Generate paraphrases for a batch of texts using Claude.\n",
    "\n",
    "    Args:\n",
    "        texts: List of financial texts to paraphrase.\n",
    "        labels: List of integer labels (ground truth).\n",
    "        n_paraphrases: Number of paraphrases per text.\n",
    "        batch_size: How many texts to send per API call.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'text' and 'label' keys.\n",
    "    \"\"\"\n",
    "    all_paraphrases = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Paraphrasing\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_labels = labels[i : i + batch_size]\n",
    "\n",
    "        # Build prompt with numbered texts\n",
    "        numbered = \"\\n\".join(\n",
    "            f\"{j+1}. [{LABEL_NAMES[lbl]}] {txt}\"\n",
    "            for j, (txt, lbl) in enumerate(zip(batch_texts, batch_labels))\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"You are a financial text paraphrasing assistant. For each numbered financial text below, generate exactly {n_paraphrases} paraphrases that:\n",
    "- Preserve the original meaning and financial sentiment\n",
    "- Use different wording, sentence structure, or phrasing\n",
    "- Stay realistic as financial text (news headlines, earnings reports, analyst commentary)\n",
    "- Keep approximately the same length\n",
    "\n",
    "Return your response as a JSON array of objects, each with \"original_index\" (1-based), \"paraphrase\" (the text), and \"label\" (the sentiment label shown in brackets).\n",
    "\n",
    "Texts to paraphrase:\n",
    "{numbered}\n",
    "\n",
    "Return ONLY valid JSON, no other text.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=4096,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            content = response.content[0].text.strip()\n",
    "            # Strip markdown code fences if present\n",
    "            if content.startswith(\"```\"):\n",
    "                content = content.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0].strip()\n",
    "\n",
    "            paraphrases = json.loads(content)\n",
    "\n",
    "            label_name_to_idx = {name: idx for idx, name in enumerate(LABEL_NAMES)}\n",
    "            for p in paraphrases:\n",
    "                idx = p[\"original_index\"] - 1  # convert to 0-based\n",
    "                lbl = label_name_to_idx.get(p[\"label\"], batch_labels[idx])\n",
    "                all_paraphrases.append({\"text\": p[\"paraphrase\"], \"label\": lbl})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at batch {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        time.sleep(0.5)  # rate limiting\n",
    "\n",
    "    return all_paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_texts = [e[\"text\"] for e in errors]\n",
    "error_labels = [e[\"true_label\"] for e in errors]\n",
    "\n",
    "print(f\"Generating {PARAPHRASES_PER_SAMPLE} paraphrases for {len(errors)} misclassified samples...\")\n",
    "paraphrased = paraphrase_batch(error_texts, error_labels, n_paraphrases=PARAPHRASES_PER_SAMPLE)\n",
    "print(f\"\\nGenerated {len(paraphrased)} paraphrases\")\n",
    "\n",
    "# Show a few examples\n",
    "for p in paraphrased[:6]:\n",
    "    print(f\"  [{LABEL_NAMES[p['label']]}] {p['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Augmented Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert paraphrased data to HF dataset with one-hot labels\n",
    "aug_texts = [p[\"text\"] for p in paraphrased]\n",
    "aug_labels = [np.eye(NUM_CLASSES)[p[\"label\"]].tolist() for p in paraphrased]\n",
    "\n",
    "aug_ds = Dataset.from_dict({\"text\": aug_texts, \"labels\": aug_labels})\n",
    "\n",
    "# Combine original training data + paraphrased augmentations\n",
    "augmented_train = concatenate_datasets([ds[\"train\"], aug_ds]).shuffle(seed=42)\n",
    "\n",
    "print(f\"Original train size:  {len(ds['train']):,}\")\n",
    "print(f\"Augmentation size:    {len(aug_ds):,}\")\n",
    "print(f\"Augmented train size: {len(augmented_train):,}\")\n",
    "print(f\"Augmentation ratio:   {len(aug_ds)/len(ds['train']):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Retrain on Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free baseline model memory\n",
    "del baseline_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Training DATABOOSTED model...\")\n",
    "boosted_model, tokenizer = train_model(\n",
    "    augmented_train, ds[\"validation\"], output_dir=\"trainer_output_boosted\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Baseline vs DataBoosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate DataBoosted model on validation\n",
    "boosted_val_preds = run_inference(boosted_model, tokenizer, val_texts)\n",
    "\n",
    "boosted_val_acc = accuracy_score(val_labels, boosted_val_preds)\n",
    "boosted_val_f1 = f1_score(val_labels, boosted_val_preds, average=\"macro\")\n",
    "\n",
    "print(f\"\\nDataBoosted validation accuracy: {boosted_val_acc:.4f}\")\n",
    "print(f\"DataBoosted validation macro F1:  {boosted_val_f1:.4f}\")\n",
    "print(classification_report(val_labels, boosted_val_preds, target_names=LABEL_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on FPB as held-out test\n",
    "fpb_50 = load_dataset(\"financial_phrasebank\", \"sentences_50agree\", trust_remote_code=True)[\"train\"]\n",
    "fpb_texts = fpb_50[\"sentence\"]\n",
    "fpb_labels = np.array(fpb_50[\"label\"])\n",
    "\n",
    "boosted_fpb_preds = run_inference(boosted_model, tokenizer, fpb_texts)\n",
    "boosted_fpb_acc = accuracy_score(fpb_labels, boosted_fpb_preds)\n",
    "boosted_fpb_f1 = f1_score(fpb_labels, boosted_fpb_preds, average=\"macro\")\n",
    "\n",
    "print(f\"\\nDataBoosted FPB accuracy: {boosted_fpb_acc:.4f}\")\n",
    "print(f\"DataBoosted FPB macro F1: {boosted_fpb_f1:.4f}\")\n",
    "print(classification_report(fpb_labels, boosted_fpb_preds, target_names=LABEL_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary comparison (including published baselines from literature)\ncomparison = pd.DataFrame([\n    # --- Published baselines (in-domain FPB train/test splits) ---\n    {\"Model\": \"LSTM+ELMo *\",          \"Split\": \"FPB 50agree\",\n     \"Accuracy\": \"0.7500\", \"Macro F1\": \"0.7000\"},\n    {\"Model\": \"ULMFit *\",             \"Split\": \"FPB 50agree\",\n     \"Accuracy\": \"0.8300\", \"Macro F1\": \"0.7900\"},\n    {\"Model\": \"ProsusAI/finbert *\",   \"Split\": \"FPB 50agree\",\n     \"Accuracy\": \"0.8600\", \"Macro F1\": \"0.8400\"},\n    {\"Model\": \"FinBERT-FinVocab *\",   \"Split\": \"FPB 50agree\",\n     \"Accuracy\": \"0.8720\", \"Macro F1\": \"—\"},\n    # --- Our models (FPB held out from training) ---\n    {\"Model\": \"Baseline (ours)\",       \"Split\": \"Validation\",\n     \"Accuracy\": f\"{val_acc:.4f}\", \"Macro F1\": f\"{val_f1:.4f}\"},\n    {\"Model\": \"DataBoosted (ours)\",    \"Split\": \"Validation\",\n     \"Accuracy\": f\"{boosted_val_acc:.4f}\", \"Macro F1\": f\"{boosted_val_f1:.4f}\"},\n    {\"Model\": \"DataBoosted (ours)\",    \"Split\": \"FPB 50agree\",\n     \"Accuracy\": f\"{boosted_fpb_acc:.4f}\", \"Macro F1\": f\"{boosted_fpb_f1:.4f}\"},\n])\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DATABOOST COMPARISON\")\nprint(\"=\" * 70)\nprint(comparison.to_string(index=False))\n\nval_delta = boosted_val_acc - val_acc\nprint(f\"\\nValidation accuracy delta: {val_delta:+.4f} ({val_delta:+.2%})\")\nprint(f\"Augmentation added {len(aug_ds)} samples ({len(aug_ds)/len(ds['train']):.1%} of train)\")\nprint(\"\\n* Published baselines trained/tested on in-domain FPB splits (Araci 2019, Yang et al. 2020)\")\nprint(\"  Our models never saw FPB during training — stricter held-out evaluation.\")\nprint(\"  See reference/fpb_benchmarks.md for full details.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. (Optional) Iterate — Round 2\n",
    "\n",
    "Re-mine errors from the DataBoosted model and augment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run a second round of DataBoost\n",
    "\n",
    "# # Mine errors from boosted model\n",
    "# errors_r2 = []\n",
    "# for i in range(len(val_texts)):\n",
    "#     if boosted_val_preds[i] != val_labels[i]:\n",
    "#         errors_r2.append({\"text\": val_texts[i], \"true_label\": int(val_labels[i])})\n",
    "#\n",
    "# print(f\"Round 2 errors: {len(errors_r2)} (down from {len(errors)})\")\n",
    "#\n",
    "# if len(errors_r2) > 0:\n",
    "#     paraphrased_r2 = paraphrase_batch(\n",
    "#         [e[\"text\"] for e in errors_r2],\n",
    "#         [e[\"true_label\"] for e in errors_r2],\n",
    "#         n_paraphrases=PARAPHRASES_PER_SAMPLE,\n",
    "#     )\n",
    "#     aug_r2 = Dataset.from_dict({\n",
    "#         \"text\": [p[\"text\"] for p in paraphrased_r2],\n",
    "#         \"labels\": [np.eye(NUM_CLASSES)[p[\"label\"]].tolist() for p in paraphrased_r2],\n",
    "#     })\n",
    "#     augmented_train_r2 = concatenate_datasets([augmented_train, aug_r2]).shuffle(seed=42)\n",
    "#\n",
    "#     del boosted_model\n",
    "#     torch.cuda.empty_cache()\n",
    "#\n",
    "#     boosted_model_r2, tokenizer = train_model(\n",
    "#         augmented_train_r2, ds[\"validation\"], output_dir=\"trainer_output_boosted_r2\"\n",
    "#     )\n",
    "#     r2_preds = run_inference(boosted_model_r2, tokenizer, val_texts)\n",
    "#     r2_acc = accuracy_score(val_labels, r2_preds)\n",
    "#     print(f\"Round 2 validation accuracy: {r2_acc:.4f} (delta: {r2_acc - val_acc:+.4f})\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}