{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experiment 3: ModernFinBERT vs Claude Opus 4.5\n\n**Aim:** Compare our fine-tuned ModernFinBERT model against Claude Opus 4.5 on the FPB test set using zero-shot classification, and contextualize against published baselines.\n\n**Key questions:**\n- How does a 149M fine-tuned specialist compare to a frontier LLM on financial sentiment?\n- What's the cost/accuracy trade-off?\n- Where do both models sit relative to published FinBERT baselines?\n\n**Published baselines** (from `reference/fpb_benchmarks.md`):\n- ProsusAI/finbert: 86% acc on FPB 50agree (Araci 2019, in-domain)\n- FinBERT-FinVocab: 87.2% acc (Yang et al. 2020, in-domain)\n- GPT-4o zero-shot: ~0.727 macro F1 on 75-99% agree subset (2025 eval paper)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic datasets scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "LABEL_NAMES = [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"]\n",
    "LABEL_MAP = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter Anthropic API key: \")\n",
    "\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpb = load_dataset(\"financial_phrasebank\", \"sentences_50agree\", trust_remote_code=True)[\"train\"]\n",
    "print(f\"FPB 50agree: {len(fpb):,} samples\")\n",
    "\n",
    "# Label distribution\n",
    "labels = fpb[\"label\"]\n",
    "for i, name in enumerate(LABEL_NAMES):\n",
    "    count = labels.count(i)\n",
    "    print(f\"  {name}: {count} ({count/len(labels):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Claude Opus 4.5 Zero-Shot Classification\n",
    "\n",
    "Send each sample to Claude with a structured prompt, collect predictions, and track token usage for cost analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a financial sentiment classifier. Given a financial text, classify its sentiment as exactly one of: POSITIVE, NEGATIVE, or NEUTRAL.\n",
    "\n",
    "Respond with ONLY a single JSON object: {\"sentiment\": \"POSITIVE\"} or {\"sentiment\": \"NEGATIVE\"} or {\"sentiment\": \"NEUTRAL\"}\n",
    "\n",
    "No other text or explanation.\"\"\"\n",
    "\n",
    "\n",
    "def classify_with_claude(texts, batch_size=20, model=\"claude-opus-4-20250514\"):\n",
    "    \"\"\"Classify texts using Claude API.\n",
    "\n",
    "    Sends texts individually to get per-sample predictions.\n",
    "    Batches are used only for progress tracking and rate limiting.\n",
    "\n",
    "    Returns:\n",
    "        predictions: list of int labels\n",
    "        total_input_tokens: total input tokens used\n",
    "        total_output_tokens: total output tokens used\n",
    "        raw_responses: list of raw response strings (for debugging)\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    raw_responses = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    for i in tqdm(range(len(texts)), desc=f\"Claude ({model})\"):\n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=50,\n",
    "                system=SYSTEM_PROMPT,\n",
    "                messages=[{\"role\": \"user\", \"content\": texts[i]}],\n",
    "            )\n",
    "\n",
    "            total_input_tokens += response.usage.input_tokens\n",
    "            total_output_tokens += response.usage.output_tokens\n",
    "\n",
    "            content = response.content[0].text.strip()\n",
    "            raw_responses.append(content)\n",
    "\n",
    "            # Parse JSON response\n",
    "            if content.startswith(\"```\"):\n",
    "                content = content.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0].strip()\n",
    "            result = json.loads(content)\n",
    "            sentiment = result[\"sentiment\"].upper()\n",
    "\n",
    "            if sentiment in (\"POSITIVE\", \"POS\"):\n",
    "                predictions.append(2)\n",
    "            elif sentiment in (\"NEGATIVE\", \"NEG\"):\n",
    "                predictions.append(0)\n",
    "            else:\n",
    "                predictions.append(1)  # NEUTRAL\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback: try to extract from raw text\n",
    "            upper = content.upper()\n",
    "            if \"POSITIVE\" in upper:\n",
    "                predictions.append(2)\n",
    "            elif \"NEGATIVE\" in upper:\n",
    "                predictions.append(0)\n",
    "            else:\n",
    "                predictions.append(1)\n",
    "            raw_responses.append(content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at sample {i}: {e}\")\n",
    "            predictions.append(1)  # default to neutral on error\n",
    "            raw_responses.append(str(e))\n",
    "\n",
    "        # Rate limiting: pause briefly every batch_size samples\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            time.sleep(1)\n",
    "\n",
    "    return predictions, total_input_tokens, total_output_tokens, raw_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will make ~4,846 API calls. Estimated cost ~$30-50 for Opus 4.5.\n",
    "# To test first, set SAMPLE_SIZE to a smaller number.\n",
    "\n",
    "SAMPLE_SIZE = None  # Set to e.g. 100 for a quick test, None for full run\n",
    "\n",
    "if SAMPLE_SIZE:\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.choice(len(fpb), SAMPLE_SIZE, replace=False)\n",
    "    test_texts = [fpb[\"sentence\"][i] for i in indices]\n",
    "    test_labels = [fpb[\"label\"][i] for i in indices]\n",
    "    print(f\"Running on {SAMPLE_SIZE} sample subset\")\n",
    "else:\n",
    "    test_texts = fpb[\"sentence\"]\n",
    "    test_labels = fpb[\"label\"]\n",
    "    print(f\"Running on full FPB ({len(test_texts):,} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_preds, input_tokens, output_tokens, raw = classify_with_claude(test_texts)\n",
    "\n",
    "print(f\"\\nTotal input tokens:  {input_tokens:,}\")\n",
    "print(f\"Total output tokens: {output_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Claude Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(test_labels)\n",
    "y_pred_claude = np.array(claude_preds)\n",
    "\n",
    "claude_acc = accuracy_score(y_true, y_pred_claude)\n",
    "claude_f1 = f1_score(y_true, y_pred_claude, average=\"macro\")\n",
    "\n",
    "print(f\"Claude Opus 4.5 — FPB 50agree\")\n",
    "print(f\"Accuracy: {claude_acc:.4f} ({int(claude_acc * len(y_true))}/{len(y_true)})\")\n",
    "print(f\"Macro F1: {claude_f1:.4f}\")\n",
    "print(f\"\\n{classification_report(y_true, y_pred_claude, target_names=LABEL_NAMES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load ModernFinBERT Results for Comparison\n",
    "\n",
    "Either run ModernFinBERT inference here, or load the pre-trained model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "mfb_tokenizer = AutoTokenizer.from_pretrained(\"neoyipeng/ModernFinBERT-base\")\n",
    "mfb_model = AutoModelForSequenceClassification.from_pretrained(\"neoyipeng/ModernFinBERT-base\")\n",
    "mfb_model = mfb_model.cuda().eval()\n",
    "\n",
    "print(f\"ModernFinBERT loaded: {sum(p.numel() for p in mfb_model.parameters()):,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ModernFinBERT on the same test samples\n",
    "mfb_preds = []\n",
    "batch_size = 32\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_texts), batch_size), desc=\"ModernFinBERT\"):\n",
    "        batch = test_texts[i : i + batch_size]\n",
    "        inputs = mfb_tokenizer(\n",
    "            batch, return_tensors=\"pt\", padding=True,\n",
    "            truncation=True, max_length=512,\n",
    "        )\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        logits = mfb_model(**inputs).logits\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        mfb_preds.extend(preds)\n",
    "\n",
    "y_pred_mfb = np.array(mfb_preds)\n",
    "\n",
    "mfb_acc = accuracy_score(y_true, y_pred_mfb)\n",
    "mfb_f1 = f1_score(y_true, y_pred_mfb, average=\"macro\")\n",
    "\n",
    "print(f\"\\nModernFinBERT — FPB 50agree\")\n",
    "print(f\"Accuracy: {mfb_acc:.4f} ({int(mfb_acc * len(y_true))}/{len(y_true)})\")\n",
    "print(f\"Macro F1: {mfb_f1:.4f}\")\n",
    "print(f\"\\n{classification_report(y_true, y_pred_mfb, target_names=LABEL_NAMES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison Table & Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Claude Opus 4.5 pricing (per 1M tokens)\nOPUS_INPUT_COST = 15.0   # $/1M input tokens\nOPUS_OUTPUT_COST = 75.0  # $/1M output tokens\n\nclaude_cost = (\n    input_tokens * OPUS_INPUT_COST / 1_000_000\n    + output_tokens * OPUS_OUTPUT_COST / 1_000_000\n)\ncost_per_sample = claude_cost / len(test_texts)\n\n# ModernFinBERT cost estimate (T4 GPU on cloud ~$0.35/hr)\n# Inference time: ~10 seconds for 4846 samples\nmfb_cost_per_hour = 0.35  # T4 on-demand\nmfb_inference_seconds = 10  # approximate\nmfb_cost = mfb_cost_per_hour * mfb_inference_seconds / 3600\nmfb_cost_per_sample = mfb_cost / len(test_texts)\n\nprint(\"=\" * 80)\nprint(\"FINAL COMPARISON: ModernFinBERT vs Claude Opus 4.5 vs Published Baselines\")\nprint(\"=\" * 80)\n\ncomparison = pd.DataFrame([\n    # --- Published baselines (from literature) ---\n    {\"Model\": \"LSTM+ELMo *\",          \"Accuracy\": \"0.7500\", \"Macro F1\": \"0.7000\",\n     \"Total Cost\": \"—\", \"Cost/Sample\": \"—\", \"Latency\": \"—\",\n     \"Note\": \"Araci 2019, in-domain\"},\n    {\"Model\": \"ULMFit *\",             \"Accuracy\": \"0.8300\", \"Macro F1\": \"0.7900\",\n     \"Total Cost\": \"—\", \"Cost/Sample\": \"—\", \"Latency\": \"—\",\n     \"Note\": \"Araci 2019, in-domain\"},\n    {\"Model\": \"ProsusAI/finbert *\",   \"Accuracy\": \"0.8600\", \"Macro F1\": \"0.8400\",\n     \"Total Cost\": \"—\", \"Cost/Sample\": \"—\", \"Latency\": \"—\",\n     \"Note\": \"Araci 2019, in-domain\"},\n    {\"Model\": \"FinBERT-FinVocab *\",   \"Accuracy\": \"0.8720\", \"Macro F1\": \"—\",\n     \"Total Cost\": \"—\", \"Cost/Sample\": \"—\", \"Latency\": \"—\",\n     \"Note\": \"Yang et al. 2020, in-domain\"},\n    # --- Our evaluations ---\n    {\"Model\": \"ModernFinBERT (149M)\",\n     \"Accuracy\": f\"{mfb_acc:.4f}\", \"Macro F1\": f\"{mfb_f1:.4f}\",\n     \"Total Cost\": f\"${mfb_cost:.4f}\", \"Cost/Sample\": f\"${mfb_cost_per_sample:.6f}\",\n     \"Latency\": \"~2ms/sample\",\n     \"Note\": \"held-out (FPB excluded)\"},\n    {\"Model\": \"Claude Opus 4.5\",\n     \"Accuracy\": f\"{claude_acc:.4f}\", \"Macro F1\": f\"{claude_f1:.4f}\",\n     \"Total Cost\": f\"${claude_cost:.2f}\", \"Cost/Sample\": f\"${cost_per_sample:.4f}\",\n     \"Latency\": \"~1-2s/sample\",\n     \"Note\": \"zero-shot\"},\n])\n\nprint(comparison[[\"Model\", \"Accuracy\", \"Macro F1\", \"Total Cost\", \"Latency\", \"Note\"]].to_string(index=False))\n\ncost_ratio = claude_cost / max(mfb_cost, 0.0001)\nprint(f\"\\nClaude is {cost_ratio:.0f}x more expensive per inference run\")\nprint(f\"Claude total cost for {len(test_texts)} samples: ${claude_cost:.2f}\")\nprint(f\"  Input tokens:  {input_tokens:,} (${input_tokens * OPUS_INPUT_COST / 1_000_000:.2f})\")\nprint(f\"  Output tokens: {output_tokens:,} (${output_tokens * OPUS_OUTPUT_COST / 1_000_000:.2f})\")\nprint(\"\\n* Published baselines trained/tested on in-domain FPB splits.\")\nprint(\"  See reference/fpb_benchmarks.md for full details.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis — Where Do They Disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find samples where the two models disagree\n",
    "disagree_mask = y_pred_mfb != y_pred_claude\n",
    "disagree_indices = np.where(disagree_mask)[0]\n",
    "\n",
    "print(f\"Models disagree on {len(disagree_indices)} / {len(y_true)} samples ({len(disagree_indices)/len(y_true):.1%})\")\n",
    "\n",
    "# Among disagreements, who is right more often?\n",
    "mfb_right = np.sum(y_pred_mfb[disagree_mask] == y_true[disagree_mask])\n",
    "claude_right = np.sum(y_pred_claude[disagree_mask] == y_true[disagree_mask])\n",
    "neither_right = np.sum(\n",
    "    (y_pred_mfb[disagree_mask] != y_true[disagree_mask])\n",
    "    & (y_pred_claude[disagree_mask] != y_true[disagree_mask])\n",
    ")\n",
    "\n",
    "print(f\"  ModernFinBERT correct: {mfb_right}\")\n",
    "print(f\"  Claude correct:        {claude_right}\")\n",
    "print(f\"  Both wrong:            {neither_right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some disagreement examples\n",
    "print(\"\\n--- Sample Disagreements ---\")\n",
    "for idx in disagree_indices[:10]:\n",
    "    true_lbl = LABEL_NAMES[y_true[idx]]\n",
    "    mfb_lbl = LABEL_NAMES[y_pred_mfb[idx]]\n",
    "    claude_lbl = LABEL_NAMES[y_pred_claude[idx]]\n",
    "    mfb_ok = \"correct\" if y_pred_mfb[idx] == y_true[idx] else \"wrong\"\n",
    "    claude_ok = \"correct\" if y_pred_claude[idx] == y_true[idx] else \"wrong\"\n",
    "    print(f\"\\nText: {test_texts[idx][:150]}...\")\n",
    "    print(f\"  True: {true_lbl} | MFB: {mfb_lbl} ({mfb_ok}) | Claude: {claude_lbl} ({claude_ok})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, (y_pred, title) in zip(axes, [\n",
    "    (y_pred_mfb, f\"ModernFinBERT\\nAcc={mfb_acc:.2%}  F1={mfb_f1:.2%}\"),\n",
    "    (y_pred_claude, f\"Claude Opus 4.5\\nAcc={claude_acc:.2%}  F1={claude_f1:.2%}\"),\n",
    "]):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=ax,\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mfb_vs_claude_confusion.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}