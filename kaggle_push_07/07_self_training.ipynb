{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7: Self-Training\n",
    "\n",
    "**Aim:** Apply self-training on top of the DataBoosted model using unlabeled financial text. This is the candidate for the final published model.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Train teacher (baseline + DataBoost from NB02 protocol)\n",
    "2. Source unlabeled financial text (~30-50K sentences)\n",
    "3. Iterative self-training (max 3 rounds) with per-class top-k selection\n",
    "4. Final evaluation comparing full progression\n",
    "\n",
    "**Anti-confirmation-bias measures:**\n",
    "- Per-class selection (prevents majority class domination)\n",
    "- Fresh student each round (no weight inheritance from teacher)\n",
    "- Validation monitoring with early stopping across rounds\n",
    "- Confidence distribution logging per round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q \"datasets>=3.4.1,<4.0.0\" scikit-learn matplotlib seaborn peft accelerate transformers anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    TrainingArguments, Trainer, AutoModelForSequenceClassification,\n",
    "    AutoTokenizer, training_args,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "LABEL_NAMES = [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"]\n",
    "FPB_SOURCE = 6\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
    "PARAPHRASES_PER_SAMPLE = 3\n",
    "\n",
    "# Self-training config\n",
    "MAX_SELF_TRAIN_ROUNDS = 3\n",
    "CONFIDENCE_THRESHOLDS = [0.15, 0.25, 0.40]  # per-class top-k percentages per round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"neoyipeng/financial_reasoning_aggregated\")\n",
    "\n",
    "label_dict = {\"NEUTRAL/MIXED\": 1, \"NEGATIVE\": 0, \"POSITIVE\": 2}\n",
    "\n",
    "ds = ds.filter(lambda x: x[\"task\"] == \"sentiment\")\n",
    "ds = ds.filter(lambda x: x[\"source\"] != FPB_SOURCE)\n",
    "\n",
    "remove_cols = [c for c in ds[\"train\"].column_names if c not in (\"text\", \"labels\")]\n",
    "ds = ds.map(\n",
    "    lambda ex: {\n",
    "        \"text\": ex[\"text\"],\n",
    "        \"labels\": np.eye(NUM_CLASSES)[label_dict[ex[\"label\"]]],\n",
    "    },\n",
    "    remove_columns=remove_cols,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(ds['train']):,}  |  Val: {len(ds['validation']):,}  |  Test: {len(ds['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpb_50 = load_dataset(\"financial_phrasebank\", \"sentences_50agree\", trust_remote_code=True)[\"train\"]\n",
    "fpb_all = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)[\"train\"]\n",
    "\n",
    "print(f\"FPB 50agree: {len(fpb_50):,} samples\")\n",
    "print(f\"FPB allAgree: {len(fpb_all):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, val_dataset, seed=3407, output_dir=\"trainer_output\", epochs=10):\n",
    "    \"\"\"Train a fresh ModernBERT-base model with LoRA and return model + tokenizer.\"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_CLASSES,\n",
    "        torch_dtype=torch.float32,\n",
    "        attn_implementation=\"sdpa\",\n",
    "    )\n",
    "    model.gradient_checkpointing_enable()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"Wqkv\", \"out_proj\", \"Wi\", \"Wo\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model = model.cuda()\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"])\n",
    "\n",
    "    train_tok = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_tok = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=8,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=10,\n",
    "            fp16=True,\n",
    "            bf16=False,\n",
    "            optim=training_args.OptimizerNames.ADAMW_TORCH,\n",
    "            learning_rate=2e-4,\n",
    "            weight_decay=0.001,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            seed=seed,\n",
    "            num_train_epochs=epochs,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            save_strategy=\"epoch\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "        compute_metrics=lambda eval_pred: {\n",
    "            \"accuracy\": accuracy_score(\n",
    "                eval_pred[1].argmax(axis=-1), eval_pred[0].argmax(axis=-1)\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model = model.cuda().eval()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, tokenizer, texts, batch_size=32):\n",
    "    \"\"\"Run inference and return predicted class indices.\"\"\"\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch, return_tensors=\"pt\", padding=True,\n",
    "                truncation=True, max_length=512,\n",
    "            )\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            logits = model(**inputs).logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "    return np.array(all_preds)\n",
    "\n",
    "\n",
    "def run_inference_with_probs(model, tokenizer, texts, batch_size=64):\n",
    "    \"\"\"Run inference and return (predicted labels, softmax probabilities).\"\"\"\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Teacher inference\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch, return_tensors=\"pt\", padding=True,\n",
    "                truncation=True, max_length=512,\n",
    "            )\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "            preds = np.argmax(probs, axis=-1)\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs)\n",
    "    return np.array(all_preds), np.array(all_probs)\n",
    "\n",
    "\n",
    "def evaluate_all(model, tokenizer):\n",
    "    \"\"\"Evaluate model on validation, test, FPB 50agree, FPB allAgree.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Validation\n",
    "    val_texts = ds[\"validation\"][\"text\"]\n",
    "    val_labels = np.argmax(ds[\"validation\"][\"labels\"], axis=1)\n",
    "    val_preds = run_inference(model, tokenizer, val_texts)\n",
    "    results[\"val_acc\"] = accuracy_score(val_labels, val_preds)\n",
    "    results[\"val_f1\"] = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "\n",
    "    # Test\n",
    "    test_texts = ds[\"test\"][\"text\"]\n",
    "    test_labels = np.argmax(ds[\"test\"][\"labels\"], axis=1)\n",
    "    test_preds = run_inference(model, tokenizer, test_texts)\n",
    "    results[\"test_acc\"] = accuracy_score(test_labels, test_preds)\n",
    "    results[\"test_f1\"] = f1_score(test_labels, test_preds, average=\"macro\")\n",
    "\n",
    "    # FPB 50agree\n",
    "    fpb50_preds = run_inference(model, tokenizer, fpb_50[\"sentence\"])\n",
    "    results[\"fpb50_acc\"] = accuracy_score(fpb_50[\"label\"], fpb50_preds)\n",
    "    results[\"fpb50_f1\"] = f1_score(fpb_50[\"label\"], fpb50_preds, average=\"macro\")\n",
    "\n",
    "    # FPB allAgree\n",
    "    fpball_preds = run_inference(model, tokenizer, fpb_all[\"sentence\"])\n",
    "    results[\"fpball_acc\"] = accuracy_score(fpb_all[\"label\"], fpball_preds)\n",
    "    results[\"fpball_f1\"] = f1_score(fpb_all[\"label\"], fpball_preds, average=\"macro\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step 1: Train Baseline Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training BASELINE model...\")\n",
    "baseline_model, tokenizer = train_model(\n",
    "    ds[\"train\"], ds[\"validation\"], output_dir=\"trainer_output_baseline\"\n",
    ")\n",
    "\n",
    "baseline_results = evaluate_all(baseline_model, tokenizer)\n",
    "print(f\"\\nBaseline results:\")\n",
    "for k, v in baseline_results.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step 2: DataBoost (Error Mining + Paraphrasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error mining on validation set\n",
    "val_texts = ds[\"validation\"][\"text\"]\n",
    "val_labels = np.argmax(ds[\"validation\"][\"labels\"], axis=1)\n",
    "val_preds = run_inference(baseline_model, tokenizer, val_texts)\n",
    "\n",
    "errors = []\n",
    "for i in range(len(val_texts)):\n",
    "    if val_preds[i] != val_labels[i]:\n",
    "        errors.append({\"text\": val_texts[i], \"true_label\": int(val_labels[i])})\n",
    "\n",
    "print(f\"Misclassified samples: {len(errors)} / {len(val_texts)} ({len(errors)/len(val_texts):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API key\n",
    "if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        os.environ[\"ANTHROPIC_API_KEY\"] = UserSecretsClient().get_secret(\"ANTHROPIC_API_KEY\")\n",
    "    except ImportError:\n",
    "        import getpass\n",
    "        os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter Anthropic API key: \")\n",
    "\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_batch(texts, labels, n_paraphrases=3, batch_size=10):\n",
    "    \"\"\"Generate paraphrases for a batch of texts using Claude.\"\"\"\n",
    "    all_paraphrases = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Paraphrasing\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_labels = labels[i : i + batch_size]\n",
    "\n",
    "        numbered = \"\\n\".join(\n",
    "            f\"{j+1}. [{LABEL_NAMES[lbl]}] {txt}\"\n",
    "            for j, (txt, lbl) in enumerate(zip(batch_texts, batch_labels))\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"You are a financial text paraphrasing assistant. For each numbered financial text below, generate exactly {n_paraphrases} paraphrases that:\n",
    "- Preserve the original meaning and financial sentiment\n",
    "- Use different wording, sentence structure, or phrasing\n",
    "- Stay realistic as financial text (news headlines, earnings reports, analyst commentary)\n",
    "- Keep approximately the same length\n",
    "\n",
    "Return your response as a JSON array of objects, each with \"original_index\" (1-based), \"paraphrase\" (the text), and \"label\" (the sentiment label shown in brackets).\n",
    "\n",
    "Texts to paraphrase:\n",
    "{numbered}\n",
    "\n",
    "Return ONLY valid JSON, no other text.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=4096,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            content = response.content[0].text.strip()\n",
    "            if content.startswith(\"```\"):\n",
    "                content = content.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0].strip()\n",
    "\n",
    "            paraphrases = json.loads(content)\n",
    "\n",
    "            label_name_to_idx = {name: idx for idx, name in enumerate(LABEL_NAMES)}\n",
    "            for p in paraphrases:\n",
    "                idx = p[\"original_index\"] - 1\n",
    "                lbl = label_name_to_idx.get(p[\"label\"], batch_labels[idx])\n",
    "                all_paraphrases.append({\"text\": p[\"paraphrase\"], \"label\": lbl})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at batch {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return all_paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_texts = [e[\"text\"] for e in errors]\n",
    "error_labels = [e[\"true_label\"] for e in errors]\n",
    "\n",
    "print(f\"Generating {PARAPHRASES_PER_SAMPLE} paraphrases for {len(errors)} misclassified samples...\")\n",
    "paraphrased = paraphrase_batch(error_texts, error_labels, n_paraphrases=PARAPHRASES_PER_SAMPLE)\n",
    "print(f\"Generated {len(paraphrased)} paraphrases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmented dataset\n",
    "aug_ds = Dataset.from_dict({\n",
    "    \"text\": [p[\"text\"] for p in paraphrased],\n",
    "    \"labels\": [np.eye(NUM_CLASSES)[p[\"label\"]].tolist() for p in paraphrased],\n",
    "})\n",
    "\n",
    "augmented_train = concatenate_datasets([ds[\"train\"], aug_ds]).shuffle(seed=42)\n",
    "\n",
    "print(f\"Original train:  {len(ds['train']):,}\")\n",
    "print(f\"Augmentation:    {len(aug_ds):,}\")\n",
    "print(f\"Augmented train: {len(augmented_train):,}\")\n",
    "\n",
    "# Free baseline model\n",
    "del baseline_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training DATABOOSTED model (teacher for self-training)...\")\n",
    "teacher_model, tokenizer = train_model(\n",
    "    augmented_train, ds[\"validation\"], output_dir=\"trainer_output_boosted\"\n",
    ")\n",
    "\n",
    "boosted_results = evaluate_all(teacher_model, tokenizer)\n",
    "print(f\"\\nDataBoosted results:\")\n",
    "for k, v in boosted_results.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Step 3: Source Unlabeled Financial Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unlabeled financial text — we ignore existing labels\n",
    "print(\"Loading unlabeled financial text sources...\")\n",
    "\n",
    "unlabeled_texts = []\n",
    "\n",
    "# Source 1: Twitter financial news sentiment (ignore labels)\n",
    "try:\n",
    "    twitter_fin = load_dataset(\"zeroshot/twitter-financial-news-sentiment\", split=\"train\")\n",
    "    unlabeled_texts.extend(twitter_fin[\"text\"])\n",
    "    print(f\"  twitter-financial-news-sentiment: {len(twitter_fin)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"  twitter-financial-news-sentiment: failed ({e})\")\n",
    "\n",
    "# Source 2: Additional twitter financial news (topic, not sentiment)\n",
    "try:\n",
    "    twitter_topic = load_dataset(\"zeroshot/twitter-financial-news-topic\", split=\"train\")\n",
    "    unlabeled_texts.extend(twitter_topic[\"text\"])\n",
    "    print(f\"  twitter-financial-news-topic: {len(twitter_topic)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"  twitter-financial-news-topic: failed ({e})\")\n",
    "\n",
    "print(f\"\\nTotal raw unlabeled: {len(unlabeled_texts):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and deduplicate\n",
    "train_text_set = set(ds[\"train\"][\"text\"])\n",
    "val_text_set = set(ds[\"validation\"][\"text\"])\n",
    "test_text_set = set(ds[\"test\"][\"text\"])\n",
    "fpb_text_set = set(fpb_50[\"sentence\"])\n",
    "known_texts = train_text_set | val_text_set | test_text_set | fpb_text_set\n",
    "\n",
    "# Filter: headline-length (5-50 words), not in training data, deduplicate\n",
    "seen = set()\n",
    "filtered_texts = []\n",
    "for text in unlabeled_texts:\n",
    "    text = text.strip()\n",
    "    word_count = len(text.split())\n",
    "    if 5 <= word_count <= 50 and text not in known_texts and text not in seen:\n",
    "        filtered_texts.append(text)\n",
    "        seen.add(text)\n",
    "\n",
    "print(f\"After filtering (5-50 words, deduplicated, no overlap): {len(filtered_texts):,}\")\n",
    "\n",
    "# Cap at 50K if needed\n",
    "if len(filtered_texts) > 50000:\n",
    "    np.random.seed(42)\n",
    "    idx = np.random.choice(len(filtered_texts), 50000, replace=False)\n",
    "    filtered_texts = [filtered_texts[i] for i in sorted(idx)]\n",
    "    print(f\"Capped at 50,000 samples\")\n",
    "\n",
    "unlabeled_pool = filtered_texts\n",
    "print(f\"Final unlabeled pool: {len(unlabeled_pool):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Step 4: Self-Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_pseudo_labels(texts, preds, probs, top_k_pct):\n",
    "    \"\"\"Select top-k% most confident predictions PER CLASS.\n",
    "\n",
    "    Returns list of dicts with 'text', 'label', 'confidence'.\n",
    "    \"\"\"\n",
    "    selected = []\n",
    "    stats = {}\n",
    "\n",
    "    for class_idx in range(NUM_CLASSES):\n",
    "        # Get all samples predicted as this class\n",
    "        class_mask = preds == class_idx\n",
    "        class_indices = np.where(class_mask)[0]\n",
    "\n",
    "        if len(class_indices) == 0:\n",
    "            stats[LABEL_NAMES[class_idx]] = {\"total\": 0, \"selected\": 0}\n",
    "            continue\n",
    "\n",
    "        # Get confidence (max probability) for each\n",
    "        class_confidences = probs[class_indices, class_idx]\n",
    "\n",
    "        # Select top-k% most confident\n",
    "        n_select = max(1, int(len(class_indices) * top_k_pct))\n",
    "        top_indices = np.argsort(class_confidences)[-n_select:]\n",
    "\n",
    "        for idx in top_indices:\n",
    "            orig_idx = class_indices[idx]\n",
    "            selected.append({\n",
    "                \"text\": texts[orig_idx],\n",
    "                \"label\": class_idx,\n",
    "                \"confidence\": float(class_confidences[idx]),\n",
    "            })\n",
    "\n",
    "        stats[LABEL_NAMES[class_idx]] = {\n",
    "            \"total\": int(len(class_indices)),\n",
    "            \"selected\": n_select,\n",
    "            \"mean_conf\": float(class_confidences[top_indices].mean()),\n",
    "            \"min_conf\": float(class_confidences[top_indices].min()),\n",
    "        }\n",
    "\n",
    "    return selected, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track progression across rounds\n",
    "progression = [\n",
    "    {\"stage\": \"Baseline\", **baseline_results},\n",
    "    {\"stage\": \"DataBoosted\", **boosted_results},\n",
    "]\n",
    "\n",
    "current_teacher = teacher_model\n",
    "current_train = augmented_train  # Start from DataBoosted training set\n",
    "best_val_acc = boosted_results[\"val_acc\"]\n",
    "round_stats = []\n",
    "\n",
    "for round_idx in range(MAX_SELF_TRAIN_ROUNDS):\n",
    "    top_k_pct = CONFIDENCE_THRESHOLDS[round_idx]\n",
    "\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"SELF-TRAINING ROUND {round_idx + 1}/{MAX_SELF_TRAIN_ROUNDS}\")\n",
    "    print(f\"Top-k selection: {top_k_pct:.0%} per class\")\n",
    "    print(f\"{'#'*60}\")\n",
    "\n",
    "    # Step 1: Teacher inference on unlabeled pool\n",
    "    preds, probs = run_inference_with_probs(current_teacher, tokenizer, unlabeled_pool)\n",
    "\n",
    "    # Step 2: Per-class top-k selection\n",
    "    pseudo_labeled, stats = select_pseudo_labels(unlabeled_pool, preds, probs, top_k_pct)\n",
    "\n",
    "    print(f\"\\nPseudo-label stats:\")\n",
    "    for cls_name, cls_stats in stats.items():\n",
    "        if cls_stats[\"total\"] > 0:\n",
    "            print(f\"  {cls_name}: {cls_stats['selected']}/{cls_stats['total']} selected \"\n",
    "                  f\"(mean conf: {cls_stats['mean_conf']:.4f}, min conf: {cls_stats['min_conf']:.4f})\")\n",
    "\n",
    "    # Check class balance\n",
    "    pseudo_class_counts = {name: sum(1 for p in pseudo_labeled if p[\"label\"] == i)\n",
    "                          for i, name in enumerate(LABEL_NAMES)}\n",
    "    total_pseudo = len(pseudo_labeled)\n",
    "    print(f\"\\nPseudo-label distribution: {pseudo_class_counts}\")\n",
    "    max_class_pct = max(pseudo_class_counts.values()) / total_pseudo if total_pseudo > 0 else 0\n",
    "    if max_class_pct > 0.70:\n",
    "        print(f\"WARNING: Class imbalance detected ({max_class_pct:.1%} single class)\")\n",
    "\n",
    "    round_stats.append({\n",
    "        \"round\": round_idx + 1,\n",
    "        \"top_k_pct\": top_k_pct,\n",
    "        \"n_pseudo\": total_pseudo,\n",
    "        \"class_distribution\": pseudo_class_counts,\n",
    "        \"stats\": stats,\n",
    "    })\n",
    "\n",
    "    # Step 3: Combine labeled + pseudo-labeled data\n",
    "    pseudo_ds = Dataset.from_dict({\n",
    "        \"text\": [p[\"text\"] for p in pseudo_labeled],\n",
    "        \"labels\": [np.eye(NUM_CLASSES)[p[\"label\"]].tolist() for p in pseudo_labeled],\n",
    "    })\n",
    "\n",
    "    combined_train = concatenate_datasets([current_train, pseudo_ds]).shuffle(seed=42)\n",
    "    print(f\"\\nCombined train size: {len(combined_train):,} \"\n",
    "          f\"(labeled: {len(current_train):,} + pseudo: {len(pseudo_ds):,})\")\n",
    "\n",
    "    # Step 4: Train fresh student\n",
    "    del current_teacher\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\nTraining STUDENT (round {round_idx + 1})...\")\n",
    "    student_model, tokenizer = train_model(\n",
    "        combined_train, ds[\"validation\"],\n",
    "        output_dir=f\"trainer_output_self_train_r{round_idx + 1}\",\n",
    "    )\n",
    "\n",
    "    # Step 5: Evaluate student\n",
    "    student_results = evaluate_all(student_model, tokenizer)\n",
    "    student_results[\"stage\"] = f\"SelfTrain R{round_idx + 1}\"\n",
    "    progression.append(student_results)\n",
    "\n",
    "    print(f\"\\nRound {round_idx + 1} results:\")\n",
    "    for k, v in student_results.items():\n",
    "        if k != \"stage\":\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "    # Step 6: Check if val accuracy improved\n",
    "    if student_results[\"val_acc\"] > best_val_acc:\n",
    "        print(f\"\\nVal accuracy improved: {best_val_acc:.4f} -> {student_results['val_acc']:.4f}\")\n",
    "        best_val_acc = student_results[\"val_acc\"]\n",
    "        current_teacher = student_model\n",
    "        current_train = combined_train\n",
    "    else:\n",
    "        print(f\"\\nVal accuracy did NOT improve: {best_val_acc:.4f} -> {student_results['val_acc']:.4f}\")\n",
    "        print(\"STOPPING self-training. Previous round's model is best.\")\n",
    "        del student_model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        break\n",
    "\n",
    "print(f\"\\nSelf-training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pseudo-Label Quality Check\n",
    "\n",
    "For the unlabeled data that originally had labels (twitter-financial-news-sentiment), compare our pseudo-labels against the original labels to measure quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pseudo-label quality against original labels (where available)\n",
    "try:\n",
    "    twitter_fin_texts = twitter_fin[\"text\"]\n",
    "    twitter_fin_labels = twitter_fin[\"label\"]\n",
    "\n",
    "    # Find overlap between pseudo-labeled and original twitter data\n",
    "    twitter_text_to_label = {t.strip(): l for t, l in zip(twitter_fin_texts, twitter_fin_labels)}\n",
    "\n",
    "    matches = 0\n",
    "    total_checked = 0\n",
    "    for p in pseudo_labeled:\n",
    "        if p[\"text\"] in twitter_text_to_label:\n",
    "            orig_label = twitter_text_to_label[p[\"text\"]]\n",
    "            if p[\"label\"] == orig_label:\n",
    "                matches += 1\n",
    "            total_checked += 1\n",
    "\n",
    "    if total_checked > 0:\n",
    "        print(f\"Pseudo-label quality check (vs original twitter labels):\")\n",
    "        print(f\"  Checked: {total_checked}\")\n",
    "        print(f\"  Agreement: {matches}/{total_checked} ({matches/total_checked:.1%})\")\n",
    "    else:\n",
    "        print(\"No overlap found for quality check.\")\n",
    "except NameError:\n",
    "    print(\"Twitter dataset not available for quality check.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Full Progression Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_df = pd.DataFrame(progression)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"FULL PROGRESSION: Baseline -> DataBoosted -> Self-Training\")\n",
    "print(\"=\" * 100)\n",
    "display_cols = [\"stage\", \"val_acc\", \"val_f1\", \"test_acc\", \"test_f1\",\n",
    "                \"fpb50_acc\", \"fpb50_f1\", \"fpball_acc\", \"fpball_f1\"]\n",
    "print(prog_df[display_cols].to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "# Deltas\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"IMPROVEMENT OVER BASELINE\")\n",
    "print(f\"{'='*100}\")\n",
    "baseline_row = prog_df.iloc[0]\n",
    "for _, row in prog_df.iloc[1:].iterrows():\n",
    "    print(f\"\\n{row['stage']}:\")\n",
    "    for metric in [\"val_acc\", \"test_acc\", \"fpb50_acc\", \"fpball_acc\"]:\n",
    "        delta = row[metric] - baseline_row[metric]\n",
    "        print(f\"  {metric}: {delta:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progression line chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "stages = prog_df[\"stage\"].values\n",
    "x = range(len(stages))\n",
    "\n",
    "# Accuracy progression\n",
    "ax = axes[0]\n",
    "ax.plot(x, prog_df[\"fpb50_acc\"], \"o-\", label=\"FPB 50agree\", color=\"#2196F3\", linewidth=2, markersize=8)\n",
    "ax.plot(x, prog_df[\"fpball_acc\"], \"s-\", label=\"FPB allAgree\", color=\"#4CAF50\", linewidth=2, markersize=8)\n",
    "ax.plot(x, prog_df[\"test_acc\"], \"^-\", label=\"Agg Test\", color=\"#FF9800\", linewidth=2, markersize=8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(stages, rotation=15, fontsize=9)\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy Progression\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 progression\n",
    "ax = axes[1]\n",
    "ax.plot(x, prog_df[\"fpb50_f1\"], \"o-\", label=\"FPB 50agree\", color=\"#2196F3\", linewidth=2, markersize=8)\n",
    "ax.plot(x, prog_df[\"fpball_f1\"], \"s-\", label=\"FPB allAgree\", color=\"#4CAF50\", linewidth=2, markersize=8)\n",
    "ax.plot(x, prog_df[\"test_f1\"], \"^-\", label=\"Agg Test\", color=\"#FF9800\", linewidth=2, markersize=8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(stages, rotation=15, fontsize=9)\n",
    "ax.set_ylabel(\"Macro F1\")\n",
    "ax.set_title(\"Macro F1 Progression\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Self-Training Progression — ModernFinBERT\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"self_training_progression.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence distribution per round\n",
    "if round_stats:\n",
    "    fig, axes = plt.subplots(1, len(round_stats), figsize=(6 * len(round_stats), 4))\n",
    "    if len(round_stats) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, rs in zip(axes, round_stats):\n",
    "        class_names = []\n",
    "        mean_confs = []\n",
    "        min_confs = []\n",
    "        for cls_name in LABEL_NAMES:\n",
    "            if cls_name in rs[\"stats\"] and rs[\"stats\"][cls_name][\"total\"] > 0:\n",
    "                class_names.append(cls_name)\n",
    "                mean_confs.append(rs[\"stats\"][cls_name][\"mean_conf\"])\n",
    "                min_confs.append(rs[\"stats\"][cls_name][\"min_conf\"])\n",
    "\n",
    "        x = np.arange(len(class_names))\n",
    "        width = 0.35\n",
    "        ax.bar(x - width/2, mean_confs, width, label=\"Mean conf\", color=\"#2196F3\")\n",
    "        ax.bar(x + width/2, min_confs, width, label=\"Min conf\", color=\"#FFA726\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(class_names)\n",
    "        ax.set_ylabel(\"Confidence\")\n",
    "        ax.set_title(f\"Round {rs['round']} (top {rs['top_k_pct']:.0%})\")\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 1.0)\n",
    "\n",
    "    plt.suptitle(\"Pseudo-Label Confidence by Round\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"self_training_confidence.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_row = prog_df.loc[prog_df[\"fpb50_acc\"].idxmax()]\n",
    "print(f\"\\nBest model (by FPB 50agree accuracy): {best_row['stage']}\")\n",
    "print(f\"  FPB 50agree:  {best_row['fpb50_acc']:.4f} acc / {best_row['fpb50_f1']:.4f} F1\")\n",
    "print(f\"  FPB allAgree: {best_row['fpball_acc']:.4f} acc / {best_row['fpball_f1']:.4f} F1\")\n",
    "print(f\"  Agg Test:     {best_row['test_acc']:.4f} acc / {best_row['test_f1']:.4f} F1\")\n",
    "\n",
    "# Overall improvement\n",
    "baseline_fpb50 = progression[0][\"fpb50_acc\"]\n",
    "best_fpb50 = best_row[\"fpb50_acc\"]\n",
    "print(f\"\\nTotal improvement over baseline (FPB 50agree): {best_fpb50 - baseline_fpb50:+.4f}\")\n",
    "\n",
    "print(f\"\\nSelf-training rounds completed: {len(round_stats)}\")\n",
    "for rs in round_stats:\n",
    "    print(f\"  Round {rs['round']}: {rs['n_pseudo']} pseudo-labels added ({rs['top_k_pct']:.0%} per class)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}