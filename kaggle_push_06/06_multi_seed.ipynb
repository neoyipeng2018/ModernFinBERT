{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6: Multi-Seed Robustness\n",
    "\n",
    "**Aim:** Run the NB01 protocol (ModernBERT+LoRA on aggregated data, held-out FPB evaluation) with 5 random seeds to get confidence intervals and demonstrate result stability.\n",
    "\n",
    "**Seeds:** `[3407, 42, 123, 456, 789]`\n",
    "\n",
    "**Per seed:** Full NB01 pipeline — train ModernBERT+LoRA on aggregated data (excluding FPB), evaluate on FPB 50agree + allAgree + aggregated test set.\n",
    "\n",
    "**Runtime:** ~5 × 40 min = ~3.5 hours on T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q \"datasets>=3.4.1,<4.0.0\" scikit-learn matplotlib seaborn peft accelerate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    TrainingArguments, Trainer, AutoModelForSequenceClassification,\n",
    "    AutoTokenizer, training_args,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "LABEL_NAMES = [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"]\n",
    "FPB_SOURCE = 6\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "SEEDS = [3407, 42, 123, 456, 789]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"neoyipeng/financial_reasoning_aggregated\")\n",
    "\n",
    "label_dict = {\"NEUTRAL/MIXED\": 1, \"NEGATIVE\": 0, \"POSITIVE\": 2}\n",
    "\n",
    "ds = ds.filter(lambda x: x[\"task\"] == \"sentiment\")\n",
    "ds = ds.filter(lambda x: x[\"source\"] != FPB_SOURCE)\n",
    "\n",
    "remove_cols = [c for c in ds[\"train\"].column_names if c not in (\"text\", \"labels\")]\n",
    "ds = ds.map(\n",
    "    lambda ex: {\n",
    "        \"text\": ex[\"text\"],\n",
    "        \"labels\": np.eye(NUM_CLASSES)[label_dict[ex[\"label\"]]],\n",
    "    },\n",
    "    remove_columns=remove_cols,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(ds['train']):,}  |  Val: {len(ds['validation']):,}  |  Test: {len(ds['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpb_50 = load_dataset(\"financial_phrasebank\", \"sentences_50agree\", trust_remote_code=True)[\"train\"]\n",
    "fpb_all = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)[\"train\"]\n",
    "\n",
    "print(f\"FPB 50agree: {len(fpb_50):,} samples\")\n",
    "print(f\"FPB allAgree: {len(fpb_all):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, val_dataset, seed, output_dir=\"trainer_output\", epochs=10):\n",
    "    \"\"\"Train a fresh ModernBERT-base model with LoRA and return model + tokenizer.\"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_CLASSES,\n",
    "        torch_dtype=torch.float32,\n",
    "        attn_implementation=\"sdpa\",\n",
    "    )\n",
    "    model.gradient_checkpointing_enable()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"Wqkv\", \"out_proj\", \"Wi\", \"Wo\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model = model.cuda()\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"])\n",
    "\n",
    "    train_tok = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_tok = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=8,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=10,\n",
    "            fp16=True,\n",
    "            bf16=False,\n",
    "            optim=training_args.OptimizerNames.ADAMW_TORCH,\n",
    "            learning_rate=2e-4,\n",
    "            weight_decay=0.001,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            seed=seed,\n",
    "            num_train_epochs=epochs,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            save_strategy=\"epoch\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "        compute_metrics=lambda eval_pred: {\n",
    "            \"accuracy\": accuracy_score(\n",
    "                eval_pred[1].argmax(axis=-1), eval_pred[0].argmax(axis=-1)\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model = model.cuda().eval()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, tokenizer, texts, batch_size=32):\n",
    "    \"\"\"Run inference and return predicted class indices.\"\"\"\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch, return_tensors=\"pt\", padding=True,\n",
    "                truncation=True, max_length=512,\n",
    "            )\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            logits = model(**inputs).logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "    return np.array(all_preds)\n",
    "\n",
    "\n",
    "def evaluate_all(model, tokenizer, ds, fpb_50, fpb_all):\n",
    "    \"\"\"Evaluate model on aggregated test set + FPB 50agree + FPB allAgree.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Aggregated test set\n",
    "    test_texts = ds[\"test\"][\"text\"]\n",
    "    test_labels = np.argmax(ds[\"test\"][\"labels\"], axis=1)\n",
    "    test_preds = run_inference(model, tokenizer, test_texts)\n",
    "    results[\"test_acc\"] = accuracy_score(test_labels, test_preds)\n",
    "    results[\"test_f1\"] = f1_score(test_labels, test_preds, average=\"macro\")\n",
    "\n",
    "    # FPB 50agree\n",
    "    fpb50_preds = run_inference(model, tokenizer, fpb_50[\"sentence\"])\n",
    "    results[\"fpb50_acc\"] = accuracy_score(fpb_50[\"label\"], fpb50_preds)\n",
    "    results[\"fpb50_f1\"] = f1_score(fpb_50[\"label\"], fpb50_preds, average=\"macro\")\n",
    "\n",
    "    # FPB allAgree\n",
    "    fpball_preds = run_inference(model, tokenizer, fpb_all[\"sentence\"])\n",
    "    results[\"fpball_acc\"] = accuracy_score(fpb_all[\"label\"], fpball_preds)\n",
    "    results[\"fpball_f1\"] = f1_score(fpb_all[\"label\"], fpball_preds, average=\"macro\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run All Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for i, seed in enumerate(SEEDS):\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"SEED {seed} ({i+1}/{len(SEEDS)})\")\n",
    "    print(f\"{'#'*60}\")\n",
    "\n",
    "    model, tokenizer = train_model(\n",
    "        ds[\"train\"], ds[\"validation\"],\n",
    "        seed=seed,\n",
    "        output_dir=f\"trainer_output_seed_{seed}\",\n",
    "    )\n",
    "\n",
    "    results = evaluate_all(model, tokenizer, ds, fpb_50, fpb_all)\n",
    "    results[\"seed\"] = seed\n",
    "    all_results.append(results)\n",
    "\n",
    "    print(f\"\\nSeed {seed} results:\")\n",
    "    print(f\"  Agg Test:     {results['test_acc']:.4f} acc / {results['test_f1']:.4f} F1\")\n",
    "    print(f\"  FPB 50agree:  {results['fpb50_acc']:.4f} acc / {results['fpb50_f1']:.4f} F1\")\n",
    "    print(f\"  FPB allAgree: {results['fpball_acc']:.4f} acc / {results['fpball_f1']:.4f} F1\")\n",
    "\n",
    "    # Cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nAll {len(SEEDS)} seeds complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df[[\"seed\", \"test_acc\", \"test_f1\", \"fpb50_acc\", \"fpb50_f1\", \"fpball_acc\", \"fpball_f1\"]]\n",
    "results_df.columns = [\"Seed\", \"Test Acc\", \"Test F1\", \"FPB50 Acc\", \"FPB50 F1\", \"FPBAll Acc\", \"FPBAll F1\"]\n",
    "\n",
    "print(\"=\" * 95)\n",
    "print(\"MULTI-SEED ROBUSTNESS — ModernBERT+LoRA on Aggregated Data\")\n",
    "print(\"=\" * 95)\n",
    "print(results_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "print(f\"\\n{'='*95}\")\n",
    "print(\"SUMMARY (mean ± std)\")\n",
    "print(f\"{'='*95}\")\n",
    "for col in [\"Test Acc\", \"Test F1\", \"FPB50 Acc\", \"FPB50 F1\", \"FPBAll Acc\", \"FPBAll F1\"]:\n",
    "    mean = results_df[col].mean()\n",
    "    std = results_df[col].std()\n",
    "    print(f\"  {col:12s}: {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "print(f\"\\n  Best FPB50 Acc:  {results_df['FPB50 Acc'].max():.4f} (seed {results_df.loc[results_df['FPB50 Acc'].idxmax(), 'Seed']})\")\n",
    "print(f\"  Worst FPB50 Acc: {results_df['FPB50 Acc'].min():.4f} (seed {results_df.loc[results_df['FPB50 Acc'].idxmin(), 'Seed']})\")\n",
    "print(f\"  Range:           {results_df['FPB50 Acc'].max() - results_df['FPB50 Acc'].min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = [\n",
    "    (\"Test Acc\", \"Test F1\", \"Aggregated Test Set\"),\n",
    "    (\"FPB50 Acc\", \"FPB50 F1\", \"FPB sentences_50agree\"),\n",
    "    (\"FPBAll Acc\", \"FPBAll F1\", \"FPB sentences_allAgree\"),\n",
    "]\n",
    "\n",
    "for ax, (acc_col, f1_col, title) in zip(axes, metrics):\n",
    "    x = np.arange(len(SEEDS))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, results_df[acc_col], width, label=\"Accuracy\", color=\"#2196F3\")\n",
    "    bars2 = ax.bar(x + width/2, results_df[f1_col], width, label=\"Macro F1\", color=\"#4CAF50\")\n",
    "\n",
    "    # Mean lines\n",
    "    acc_mean = results_df[acc_col].mean()\n",
    "    f1_mean = results_df[f1_col].mean()\n",
    "    ax.axhline(y=acc_mean, color=\"#1565C0\", linestyle=\"--\", alpha=0.7, label=f\"Acc mean: {acc_mean:.4f}\")\n",
    "    ax.axhline(y=f1_mean, color=\"#2E7D32\", linestyle=\"--\", alpha=0.7, label=f\"F1 mean: {f1_mean:.4f}\")\n",
    "\n",
    "    ax.set_xlabel(\"Seed\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(SEEDS, fontsize=8)\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.set_ylim(0.85, 1.0)\n",
    "\n",
    "plt.suptitle(\"Multi-Seed Robustness — ModernBERT+LoRA\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"multi_seed_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for distribution across seeds\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "box_data = {\n",
    "    \"Test\\nAcc\": results_df[\"Test Acc\"].values,\n",
    "    \"Test\\nF1\": results_df[\"Test F1\"].values,\n",
    "    \"FPB50\\nAcc\": results_df[\"FPB50 Acc\"].values,\n",
    "    \"FPB50\\nF1\": results_df[\"FPB50 F1\"].values,\n",
    "    \"FPBAll\\nAcc\": results_df[\"FPBAll Acc\"].values,\n",
    "    \"FPBAll\\nF1\": results_df[\"FPBAll F1\"].values,\n",
    "}\n",
    "\n",
    "bp = ax.boxplot(\n",
    "    box_data.values(),\n",
    "    labels=box_data.keys(),\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor=\"#90CAF9\"),\n",
    ")\n",
    "\n",
    "# Overlay individual points\n",
    "for i, (name, data) in enumerate(box_data.items()):\n",
    "    ax.scatter([i + 1] * len(data), data, color=\"#1565C0\", s=30, zorder=3)\n",
    "\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(f\"Score Distribution Across {len(SEEDS)} Seeds\")\n",
    "ax.set_ylim(0.85, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"multi_seed_boxplot.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Summary for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PAPER-READY RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModernBERT-base + LoRA trained on aggregated financial data (FPB held out)\")\n",
    "print(f\"Results over {len(SEEDS)} seeds: {SEEDS}\")\n",
    "print()\n",
    "\n",
    "for col_pair, name in [\n",
    "    ((\"FPB50 Acc\", \"FPB50 F1\"), \"FPB sentences_50agree\"),\n",
    "    ((\"FPBAll Acc\", \"FPBAll F1\"), \"FPB sentences_allAgree\"),\n",
    "    ((\"Test Acc\", \"Test F1\"), \"Aggregated test set\"),\n",
    "]:\n",
    "    acc_col, f1_col = col_pair\n",
    "    acc_mean = results_df[acc_col].mean()\n",
    "    acc_std = results_df[acc_col].std()\n",
    "    f1_mean = results_df[f1_col].mean()\n",
    "    f1_std = results_df[f1_col].std()\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Accuracy: {acc_mean:.2f}% ± {acc_std:.2f}%\")\n",
    "    print(f\"    Macro F1: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}